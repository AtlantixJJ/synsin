{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Interactive Demo Notebook</h1>\n",
    "\n",
    "Notebook for visualising models interactively on the given datasets. \n",
    "    After running a cell, use a cursor to interact with the image.\n",
    "    One can translate the image around or orbit it. \n",
    "\n",
    "Run either the rotation or translation separately. Note that we've only tested this for our synsin models, not the baselines.\n",
    "\n",
    "Note that only the most recently run cell is interactive (so if you run all cells, only the rotation example will be interactive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import ipywidgets as wdg\n",
    "import quaternion\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "os.environ['DEBUG'] = '0'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.networks.sync_batchnorm import convert_model\n",
    "from models.base_model import BaseModel\n",
    "\n",
    "from options.options import get_model, get_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Set up the models </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = True\n",
    "\n",
    "# REALESTATE\n",
    "MODEL_PATH = './modelcheckpoints/realestate/zbufferpts.pth'\n",
    "\n",
    "#MP3D\n",
    "#MODEL_PATH = './modelcheckpoints/mp3d/zbufferpts.pth'\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# Set to true if you want to use a pre saved image as opposed to samples\n",
    "# from the test of the dataset used to train the model\n",
    "USE_IMAGE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model %s ... \n",
      "RESNET encoder\n",
      "RESNET decoder\n",
      "['1.0_l1', '10.0_content']\n",
      "<zip object at 0x7f4a9030b0f0>\n",
      "[0, 1, 2]\n",
      "Loaded model\n"
     ]
    }
   ],
   "source": [
    "opts = torch.load(MODEL_PATH)['opts']\n",
    "opts.render_ids = [1]\n",
    "\n",
    "model = get_model(opts)\n",
    "\n",
    "torch_devices = [int(gpu_id.strip()) for gpu_id in opts.gpu_ids.split(\",\")]\n",
    "print(torch_devices)\n",
    "device = 'cuda:' + str(torch_devices[0])\n",
    "\n",
    "if 'sync' in opts.norm_G:\n",
    "    model = convert_model(model)\n",
    "    model = nn.DataParallel(model, torch_devices[0:1]).cuda()\n",
    "else:\n",
    "    model = nn.DataParallel(model, torch_devices[0:1]).cuda()\n",
    "\n",
    "\n",
    "#  Load the original model to be tested\n",
    "model_to_test = BaseModel(model, opts)\n",
    "model_to_test.load_state_dict(torch.load(MODEL_PATH)['state_dict'])\n",
    "model_to_test.eval()\n",
    "\n",
    "print(\"Loaded model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Load the dataset </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset realestate ...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "/checkpoint/ow045820/data/realestate10K/RealEstate10K//frames/train/video_loc.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-619a6300d363>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mDatasetTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     dataloader = DataLoader(data, shuffle=False, drop_last=False, \n\u001b[1;32m     21\u001b[0m                             batch_size=1, num_workers=0, pin_memory=True)\n",
      "\u001b[0;32m/data1/jianjin/synsin/data/realestate10k.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, opts, num_views, seed, vectorize)\u001b[0m\n\u001b[1;32m     23\u001b[0m         self.imageset = np.loadtxt(\n\u001b[1;32m     24\u001b[0m             \u001b[0mopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/frames/%s/video_loc.txt\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         )\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/synsin/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/synsin/lib/python3.7/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/synsin/lib/python3.7/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    621\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: /checkpoint/ow045820/data/realestate10K/RealEstate10K//frames/train/video_loc.txt not found."
     ]
    }
   ],
   "source": [
    "if USE_IMAGE:\n",
    "    # Load the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256,256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    im = Image.open('./demos/im.jpg')\n",
    "    im = transform(im)\n",
    "    \n",
    "    batch = {\n",
    "        'images' : [im.unsqueeze(0)],\n",
    "        'cameras' : [{\n",
    "            'K' : torch.eye(4).unsqueeze(0),\n",
    "            'Kinv' : torch.eye(4).unsqueeze(0)}]}\n",
    "else:\n",
    "    DatasetTrain = get_dataset(opts)\n",
    "\n",
    "    data = DatasetTrain('test', opts, vectorize=False)\n",
    "    dataloader = DataLoader(data, shuffle=False, drop_last=False, \n",
    "                            batch_size=1, num_workers=0, pin_memory=True)\n",
    "\n",
    "    iter_dataloader = iter(dataloader)\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        batch = next(iter_dataloader)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_imgs = model_to_test.model.module.forward_angle(batch, [torch.eye(4).unsqueeze(0).repeat(1,1,1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Run the translation example </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "# Create and display textarea widget\n",
    "txt = wdg.Textarea(\n",
    "    value='',\n",
    "    placeholder='',\n",
    "    description='event:',\n",
    "    disabled=False\n",
    ")\n",
    "display(txt)\n",
    "\n",
    "pred_imshow = plt.imshow(pred_imgs[0].squeeze().cpu().permute(1,2,0).numpy() * 0.5 + 0.5)\n",
    "\n",
    "import time\n",
    "\n",
    "class MouseEvent:\n",
    "    def __init__(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        \n",
    "        self.clicked = False\n",
    "        \n",
    "        self.prev_time = 0\n",
    "        \n",
    "    def onclick(self, event):\n",
    "        self.clicked = True\n",
    "\n",
    "    def onmotion(self, event):\n",
    "        if not(self.clicked):\n",
    "            return\n",
    "        \n",
    "        delta_time = time.time() - self.prev_time \n",
    "        \n",
    "        if delta_time < 0.25:\n",
    "            return\n",
    "        \n",
    "        txt.value = 'Delta time'\n",
    "        \n",
    "        self.prev_time = time.time()\n",
    "        \n",
    "        rel_value = '(%0.4f,%0.4f) released' % (event.xdata, event.ydata)\n",
    "        dx = (event.xdata - 128.) / 256.\n",
    "        dy = (event.ydata - 128.) / 256.\n",
    "    \n",
    "        # Visualise the scene at the new position\n",
    "        RTs = torch.eye(4)\n",
    "        RTs[0,3] = dx\n",
    "        RTs[1,3] = dy\n",
    "        \n",
    "        txt.value = \"(%0.4f, %0.4f) :: (%0.4f, %0.4f)\" % (event.xdata, event.ydata, dx, dy)\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_imgs = model_to_test.model.module.forward_angle(batch, [RTs.unsqueeze(0)])\n",
    "        pred_imshow.set_data(pred_imgs[0].squeeze().cpu().permute(1,2,0).numpy() * 0.5 + 0.5)\n",
    "        plt.draw()\n",
    "        \n",
    "    def onrelease(self, event):\n",
    "        self.clicked = False\n",
    "        RTs = torch.eye(4)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_imgs = model_to_test.model.module.forward_angle(batch, [RTs.unsqueeze(0)])\n",
    "        pred_imshow.set_data(pred_imgs[0].squeeze().cpu().permute(1,2,0).numpy() * 0.5 + 0.5)\n",
    "        plt.draw()\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "event_tracker = MouseEvent()\n",
    "\n",
    "# Create an hard reference to the callback not to be cleared by the garbage collector\n",
    "bp = fig.canvas.mpl_connect('button_press_event', event_tracker.onclick)\n",
    "br = fig.canvas.mpl_connect('button_release_event', event_tracker.onrelease)\n",
    "br = fig.canvas.mpl_connect('motion_notify_event', event_tracker.onmotion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Run the rotation example </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "# Create and display textarea widget\n",
    "txt = wdg.Textarea(\n",
    "    value='',\n",
    "    placeholder='',\n",
    "    description='event:',\n",
    "    disabled=False\n",
    ")\n",
    "display(txt)\n",
    "\n",
    "pred_imshow = plt.imshow(pred_imgs[0].squeeze().cpu().permute(1,2,0).numpy() * 0.5 + 0.5)\n",
    "\n",
    "import time\n",
    "\n",
    "class MouseEvent:\n",
    "    def __init__(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "        \n",
    "        self.clicked = False\n",
    "        \n",
    "        self.prev_time = 0\n",
    "        \n",
    "    def onclick(self, event):\n",
    "        self.clicked = True\n",
    "        txt.value='Clicked'\n",
    "\n",
    "    def onmotion(self, event):\n",
    "        if not(self.clicked):\n",
    "            return\n",
    "        \n",
    "        delta_time = time.time() - self.prev_time \n",
    "        \n",
    "        if delta_time < 0.25:\n",
    "            return\n",
    "        \n",
    "        \n",
    "        self.prev_time = time.time()\n",
    "        \n",
    "        dx = (event.xdata - 128.) * 0.001\n",
    "        dy = (event.ydata - 128.) * 0.00005\n",
    "        \n",
    "        \n",
    "        flip_y = dy < 0\n",
    "        dy = 1 - dy\n",
    "        \n",
    "        theta = np.arcsin(dx)\n",
    "        \n",
    "        if flip_y:\n",
    "            phi = - np.arccos(1 + (1 - dy))\n",
    "        else:\n",
    "            phi = np.arccos(dy)\n",
    "            \n",
    "        # Visualise the scene at the new position\n",
    "        RTs = torch.eye(4)\n",
    "        \n",
    "        RTs[0:3,0:3] = torch.Tensor(quaternion.as_rotation_matrix(quaternion.from_rotation_vector([phi, theta, 0])))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_imgs = model_to_test.model.module.forward_angle(batch, [RTs.unsqueeze(0)])\n",
    "        pred_imshow.set_data(pred_imgs[0].squeeze().cpu().permute(1,2,0).numpy() * 0.5 + 0.5)\n",
    "        plt.draw()\n",
    "        \n",
    "    def onrelease(self, event):\n",
    "        self.clicked = False\n",
    "        txt.value='Release clicked'\n",
    "        RTs = torch.eye(4)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_imgs = model_to_test.model.module.forward_angle(batch, [RTs.unsqueeze(0)])\n",
    "        pred_imshow.set_data(pred_imgs[0].squeeze().cpu().permute(1,2,0).numpy() * 0.5 + 0.5)\n",
    "        plt.draw()\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "event_trackerRotate = MouseEvent()\n",
    "\n",
    "# Create an hard reference to the callback not to be cleared by the garbage collector\n",
    "bp = fig.canvas.mpl_connect('button_press_event', event_trackerRotate.onclick)\n",
    "br = fig.canvas.mpl_connect('button_release_event', event_trackerRotate.onrelease)\n",
    "br = fig.canvas.mpl_connect('motion_notify_event', event_trackerRotate.onmotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
